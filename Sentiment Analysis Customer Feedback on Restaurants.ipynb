{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da810ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1900af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"zomato.csv\")\n",
    "data_review = dataset['reviews_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063b1fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [('Rated 4.0', 'RATED\\n  A beautiful place to ...\n",
       "1        [('Rated 4.0', 'RATED\\n  Had been here for din...\n",
       "2        [('Rated 3.0', \"RATED\\n  Ambience is not that ...\n",
       "3        [('Rated 4.0', \"RATED\\n  Great food and proper...\n",
       "4        [('Rated 4.0', 'RATED\\n  Very good restaurant ...\n",
       "                               ...                        \n",
       "51712    [('Rated 5.0', \"RATED\\n  Food and service are ...\n",
       "51713                                                   []\n",
       "51714                                                   []\n",
       "51715    [('Rated 4.0', 'RATED\\n  Nice and friendly pla...\n",
       "51716    [('Rated 5.0', 'RATED\\n  Great ambience , look...\n",
       "Name: reviews_list, Length: 51717, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3396abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for row_num in range(51717):\n",
    "    lst = data_review[row_num].split(\"('\")\n",
    "    for i in lst:\n",
    "        if len(i) > 5:\n",
    "            if i.find(\"',\") != -1:\n",
    "                single_rev = i.split(\"',\")\n",
    "                if len(single_rev[0]) > 2:\n",
    "                    x.append(single_rev[0])\n",
    "                if len(single_rev[1]) > 2:\n",
    "                    y.append(single_rev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb53d9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "rating_final = []\n",
    "review_final = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7625b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for loop in range(40000):\n",
    "    data_x = x[loop]\n",
    "    data_x = re.sub('[a-zA-Z]',\" \",data_x)\n",
    "    data_x = data_x.split()\n",
    "    data_x = ''.join(data_x)\n",
    "    data_x = float(data_x)\n",
    "    if data_x < 2.5:\n",
    "        rating_final.append(\"poor\")\n",
    "    elif data_x >= 2.5 and data_x <= 3.5:\n",
    "        rating_final.append(\"average\")\n",
    "    elif data_x > 3.5:\n",
    "        rating_final.append(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebcf73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "rating_final = le.fit_transform(rating_final)\n",
    "rating_final = np.array(rating_final)\n",
    "rating_final = np.expand_dims(rating_final, axis=1)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one = OneHotEncoder()\n",
    "rates = one.fit_transform(rating_final).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a88bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loop in range(40000):\n",
    "    data_y = y[loop]\n",
    "    data_y = re.sub('[^a-zA-Z]',\" \",data_y)\n",
    "    data_y = data_y.lower()\n",
    "    data_y = data_y.split()\n",
    "    data_y = [ps.stem(word) for word in data_y if not word in set(stopwords.words('english'))]\n",
    "    data_y = ' '.join(data_y)\n",
    "    review_final.append(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5113838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 20000)\n",
    "x_final = cv.fit_transform(review_final).toarray()\n",
    "\n",
    "import pickle\n",
    "pickle.dump(cv, open('cv.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35fa7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_final, rates, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04598062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 393s 2s/step - loss: 0.4973 - mae: 0.1589 - acc: 0.8322\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 387s 2s/step - loss: 0.1388 - mae: 0.0482 - acc: 0.9537\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 362s 1s/step - loss: 0.0923 - mae: 0.0322 - acc: 0.9693\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 358s 1s/step - loss: 0.0815 - mae: 0.0288 - acc: 0.9722\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 380s 2s/step - loss: 0.0753 - mae: 0.0269 - acc: 0.9739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231da607640>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 13264, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 3, kernel_initializer = 'random_uniform', activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=[\"mae\", \"acc\"])\n",
    "model.fit(x_train, y_train, batch_size = 128, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c182580",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "text = \"the food is okay. average place \"\n",
    "text = re.sub('[^a-zA-Z]',' ',text)\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "text = ' '.join(text)\n",
    "\n",
    "y_p = model.predict(cv.transform([text]))\n",
    "model.save(\"zomato_2_analysis.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
